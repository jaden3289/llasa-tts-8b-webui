# MCP (Model Context Protocol) ä½¿ç”¨æŒ‡å—

## ğŸ“– ä»€ä¹ˆæ˜¯ MCP?

**Model Context Protocol (MCP)** æ˜¯ä¸€ä¸ªç¨‹åºåŒ–è®¿é—®æ¥å£ï¼Œå…è®¸å…¶ä»–åº”ç”¨ç¨‹åºæˆ– AI åŠ©æ‰‹ç›´æ¥è°ƒç”¨æœ¬é¡¹ç›®çš„åŠŸèƒ½ã€‚

ä¸ä¼ ç»Ÿçš„ REST API ç›¸æ¯”ï¼ŒMCP æä¾›ï¼š
- âœ… æ›´ç®€æ´çš„å·¥å…·å®šä¹‰
- âœ… å†…ç½®ç±»å‹æ£€æŸ¥
- âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†
- âœ… é€‚åˆ AI Agent é›†æˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ MCP æœåŠ¡å™¨

MCP æœåŠ¡å™¨å·²é›†æˆåœ¨ Docker å®¹å™¨ä¸­ï¼š

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ˆåŒ…æ‹¬ MCPï¼‰
./start.sh

# MCP æœåŠ¡å™¨å°†åœ¨å®¹å™¨å†…è¿è¡Œ
```

### 2. é…ç½® MCP å®¢æˆ·ç«¯

åˆ›å»º MCP é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ `.mcp/config.json`ï¼‰ï¼š

```json
{
  "mcpServers": {
    "llasa-tts": {
      "command": "docker",
      "args": [
        "exec",
        "-i",
        "llasa-tts-8b-webui",
        "python",
        "/app/mcp_server.py"
      ],
      "env": {
        "GPU_IDLE_TIMEOUT": "600"
      }
    }
  }
}
```

### 3. ä½¿ç”¨ MCP å·¥å…·

åœ¨æ”¯æŒ MCP çš„å®¢æˆ·ç«¯ä¸­ä½¿ç”¨å·¥å…·ï¼š

```python
from mcp import ClientSession

async with ClientSession() as session:
    # è°ƒç”¨å·¥å…·
    result = await session.call_tool("generate_speech", {
        "audio_path": "/path/to/reference.wav",
        "ref_text": "è¿™æ˜¯å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬",
        "target_text": "è¿™æ˜¯è¦ç”Ÿæˆçš„æ–‡æœ¬"
    })

    print(result)
```

## ğŸ› ï¸ å¯ç”¨å·¥å…·

### 1. generate_speech - ç”Ÿæˆè¯­éŸ³

**æè¿°**ï¼šæ ¹æ®å‚è€ƒéŸ³é¢‘ç”Ÿæˆç›®æ ‡æ–‡æœ¬çš„è¯­éŸ³ï¼ˆè¯­éŸ³å…‹éš†/TTSï¼‰

**å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `audio_path` | string | âœ… | - | å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„ |
| `ref_text` | string | âœ… | - | å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹ |
| `target_text` | string | âœ… | - | è¦ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ |
| `output_path` | string | âŒ | è‡ªåŠ¨ç”Ÿæˆ | è¾“å‡ºæ–‡ä»¶è·¯å¾„ |
| `system_prompt` | string | âŒ | "Convert the text to speech" | ç³»ç»Ÿæç¤ºè¯ |
| `sample_rate` | int | âŒ | 24000 | é‡‡æ ·ç‡ï¼ˆ16000, 22050, 24000, 28000, 32000ï¼‰ |
| `temperature` | float | âŒ | 1.0 | æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.5ï¼‰ |
| `top_k` | int | âŒ | 50 | Top-K é‡‡æ ·ï¼ˆ1-100ï¼‰ |
| `top_p` | float | âŒ | 0.9 | Nucleus é‡‡æ ·ï¼ˆ0.0-1.0ï¼‰ |
| `penalty` | float | âŒ | 1.2 | é‡å¤æƒ©ç½šï¼ˆ0.0-2.0ï¼‰ |
| `random_seed` | int | âŒ | 49 | éšæœºç§å­ |

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "output_path": "/app/outputs/generated_1234567890.wav",
  "sample_rate": 24000,
  "duration_seconds": 5.2
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("generate_speech", {
    "audio_path": "/app/temp/reference.wav",
    "ref_text": "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨è¯­éŸ³åˆæˆç³»ç»Ÿ",
    "target_text": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
    "temperature": 1.0,
    "sample_rate": 24000
})

if result["status"] == "success":
    print(f"ç”ŸæˆæˆåŠŸ: {result['output_path']}")
    print(f"æ—¶é•¿: {result['duration_seconds']} ç§’")
```

### 2. transcribe_audio - è½¬å½•éŸ³é¢‘

**æè¿°**ï¼šå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºæ–‡å­—ï¼ˆASRï¼‰

**å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `audio_path` | string | âœ… | éŸ³é¢‘æ–‡ä»¶è·¯å¾„ |

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "text": "è¿™æ˜¯è½¬å½•çš„æ–‡å­—å†…å®¹",
  "language": "zh"
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("transcribe_audio", {
    "audio_path": "/app/temp/audio.wav"
})

if result["status"] == "success":
    print(f"è½¬å½•ç»“æœ: {result['text']}")
    print(f"è¯­è¨€: {result['language']}")
```

### 3. get_gpu_status - è·å– GPU çŠ¶æ€

**æè¿°**ï¼šæŸ¥è¯¢å½“å‰ GPU å’Œæ¨¡å‹çŠ¶æ€

**å‚æ•°**ï¼šæ— 

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "data": {
    "Llasa-8B": {
      "model_name": "Llasa-8B",
      "location": "CPU",
      "idle_time_seconds": 123.4,
      "idle_timeout_seconds": 600,
      "monitor_running": true,
      "gpu_memory": {
        "GPU_0": {
          "allocated_gb": 0.5,
          "reserved_gb": 0.6,
          "total_gb": 24.0
        }
      }
    }
  }
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("get_gpu_status", {})

if result["status"] == "success":
    for model_name, status in result["data"].items():
        print(f"{model_name}: {status['location']}")
        print(f"  ç©ºé—²æ—¶é—´: {status['idle_time_seconds']} ç§’")
```

### 4. offload_gpu - å¸è½½æ¨¡å‹åˆ° CPU

**æè¿°**ï¼šæ‰‹åŠ¨å°†æ‰€æœ‰æ¨¡å‹ä» GPU å¸è½½åˆ° CPUï¼Œé‡Šæ”¾ GPU æ˜¾å­˜

**å‚æ•°**ï¼šæ— 

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "message": "æ‰€æœ‰æ¨¡å‹å·²å¸è½½åˆ° CPUï¼ŒGPU æ˜¾å­˜å·²é‡Šæ”¾"
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("offload_gpu", {})
print(result["message"])
```

### 5. release_gpu - å®Œå…¨é‡Šæ”¾æ¨¡å‹

**æè¿°**ï¼šå®Œå…¨é‡Šæ”¾æ‰€æœ‰æ¨¡å‹ï¼ˆæ¸…ç©º GPU å’Œ CPU ç¼“å­˜ï¼‰

**å‚æ•°**ï¼šæ— 

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "message": "æ‰€æœ‰æ¨¡å‹å·²å®Œå…¨é‡Šæ”¾"
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("release_gpu", {})
print(result["message"])
```

### 6. update_gpu_timeout - æ›´æ–°è¶…æ—¶æ—¶é—´

**æè¿°**ï¼šæ›´æ–° GPU ç©ºé—²è¶…æ—¶æ—¶é—´

**å‚æ•°**ï¼š

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `timeout_seconds` | int | âœ… | è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |

**è¿”å›å€¼**ï¼š

```json
{
  "status": "success",
  "message": "GPU ç©ºé—²è¶…æ—¶å·²æ›´æ–°ä¸º 300 ç§’"
}
```

**ç¤ºä¾‹**ï¼š

```python
result = await session.call_tool("update_gpu_timeout", {
    "timeout_seconds": 300
})
print(result["message"])
```

## ğŸ“ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ‰¹é‡è¯­éŸ³ç”Ÿæˆ

```python
import asyncio
from mcp import ClientSession
from pathlib import Path

async def batch_generate(texts, reference_audio, reference_text):
    """æ‰¹é‡ç”Ÿæˆè¯­éŸ³"""
    async with ClientSession() as session:
        tasks = []
        for i, text in enumerate(texts):
            task = session.call_tool("generate_speech", {
                "audio_path": reference_audio,
                "ref_text": reference_text,
                "target_text": text,
                "output_path": f"/app/outputs/batch_{i}.wav"
            })
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œï¼ˆæ³¨æ„ï¼šå®é™…ä¼šå— GPU é™åˆ¶ï¼‰
        results = await asyncio.gather(*tasks)

        return results

# ä½¿ç”¨
texts = [
    "ç¬¬ä¸€æ®µè¦ç”Ÿæˆçš„æ–‡å­—",
    "ç¬¬äºŒæ®µè¦ç”Ÿæˆçš„æ–‡å­—",
    "ç¬¬ä¸‰æ®µè¦ç”Ÿæˆçš„æ–‡å­—"
]

results = asyncio.run(batch_generate(
    texts=texts,
    reference_audio="/app/temp/reference.wav",
    reference_text="å‚è€ƒéŸ³é¢‘çš„æ–‡å­—"
))

for i, result in enumerate(results):
    if result["status"] == "success":
        print(f"âœ… ç¬¬ {i+1} æ®µç”ŸæˆæˆåŠŸ: {result['output_path']}")
    else:
        print(f"âŒ ç¬¬ {i+1} æ®µç”Ÿæˆå¤±è´¥: {result['error']}")
```

### åœºæ™¯ 2: è‡ªåŠ¨è½¬å½• + ç”Ÿæˆ

```python
async def transcribe_and_generate(audio_file, target_text):
    """å…ˆè½¬å½•å‚è€ƒéŸ³é¢‘ï¼Œå†ç”Ÿæˆç›®æ ‡è¯­éŸ³"""
    async with ClientSession() as session:
        # 1. è½¬å½•å‚è€ƒéŸ³é¢‘
        transcribe_result = await session.call_tool("transcribe_audio", {
            "audio_path": audio_file
        })

        if transcribe_result["status"] != "success":
            return {"error": "è½¬å½•å¤±è´¥"}

        ref_text = transcribe_result["text"]
        print(f"è½¬å½•ç»“æœ: {ref_text}")

        # 2. ç”Ÿæˆç›®æ ‡è¯­éŸ³
        generate_result = await session.call_tool("generate_speech", {
            "audio_path": audio_file,
            "ref_text": ref_text,
            "target_text": target_text
        })

        return generate_result

# ä½¿ç”¨
result = asyncio.run(transcribe_and_generate(
    audio_file="/app/temp/my_voice.wav",
    target_text="è¿™æ˜¯æˆ‘æƒ³è¯´çš„è¯"
))

if result["status"] == "success":
    print(f"ç”ŸæˆæˆåŠŸ: {result['output_path']}")
```

### åœºæ™¯ 3: æ™ºèƒ½èµ„æºç®¡ç†

```python
async def smart_batch_processing(audio_files):
    """æ™ºèƒ½æ‰¹é‡å¤„ç†ï¼Œè‡ªåŠ¨ç®¡ç† GPU èµ„æº"""
    async with ClientSession() as session:
        results = []

        for i, audio_file in enumerate(audio_files):
            # 1. å¤„ç†æ–‡ä»¶
            result = await session.call_tool("transcribe_audio", {
                "audio_path": audio_file
            })
            results.append(result)

            # 2. æ¯å¤„ç† 10 ä¸ªæ–‡ä»¶ï¼Œæ‰‹åŠ¨å¸è½½ä¸€æ¬¡
            if (i + 1) % 10 == 0:
                await session.call_tool("offload_gpu", {})
                print(f"å·²å¤„ç† {i+1} ä¸ªæ–‡ä»¶ï¼ŒGPU å·²å¸è½½")

        # 3. å…¨éƒ¨å®Œæˆåï¼Œå®Œå…¨é‡Šæ”¾
        await session.call_tool("release_gpu", {})
        print("å…¨éƒ¨å¤„ç†å®Œæˆï¼ŒGPU å·²é‡Šæ”¾")

        return results
```

### åœºæ™¯ 4: ç›‘æ§å’Œå‘Šè­¦

```python
async def monitor_gpu():
    """ç›‘æ§ GPU çŠ¶æ€ï¼Œè¶…è¿‡é˜ˆå€¼æ—¶å‘Šè­¦"""
    async with ClientSession() as session:
        while True:
            # è·å– GPU çŠ¶æ€
            result = await session.call_tool("get_gpu_status", {})

            if result["status"] == "success":
                data = result["data"]

                for model_name, status in data.items():
                    gpu_mem = status["gpu_memory"].get("GPU_0", {})
                    allocated = gpu_mem.get("allocated_gb", 0)
                    total = gpu_mem.get("total_gb", 24)

                    # æ˜¾å­˜ä½¿ç”¨ç‡è¶…è¿‡ 90%
                    usage_ratio = allocated / total
                    if usage_ratio > 0.9:
                        print(f"âš ï¸  è­¦å‘Š: {model_name} æ˜¾å­˜ä½¿ç”¨ç‡ {usage_ratio*100:.1f}%")

                        # å°è¯•å¸è½½
                        await session.call_tool("offload_gpu", {})
                        print("å·²å°è¯•å¸è½½æ¨¡å‹")

            # æ¯ 30 ç§’æ£€æŸ¥ä¸€æ¬¡
            await asyncio.sleep(30)
```

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰ MCP æœåŠ¡å™¨ä½ç½®

å¦‚æœæ‚¨åœ¨é Docker ç¯å¢ƒä¸­è¿è¡Œï¼š

```json
{
  "mcpServers": {
    "llasa-tts": {
      "command": "python",
      "args": ["/path/to/mcp_server.py"],
      "env": {
        "GPU_IDLE_TIMEOUT": "600",
        "LLASA_MODEL_PATH": "/path/to/models/Llasa-8B",
        "XCODEC_MODEL_PATH": "/path/to/models/xcodec2"
      }
    }
  }
}
```

### ä½¿ç”¨è¿œç¨‹ MCP æœåŠ¡å™¨

é€šè¿‡ SSH è¿æ¥è¿œç¨‹æœåŠ¡å™¨ï¼š

```json
{
  "mcpServers": {
    "llasa-tts-remote": {
      "command": "ssh",
      "args": [
        "user@remote-server",
        "docker exec -i llasa-tts-8b-webui python /app/mcp_server.py"
      ]
    }
  }
}
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: MCP æœåŠ¡å™¨æ— æ³•è¿æ¥

**ç—‡çŠ¶**ï¼šå®¢æˆ·ç«¯è¿æ¥è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# 1. æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker ps | grep llasa

# 2. æ£€æŸ¥ MCP æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
docker exec llasa-tts-8b-webui ps aux | grep mcp

# 3. æ‰‹åŠ¨æµ‹è¯• MCP æœåŠ¡å™¨
docker exec -it llasa-tts-8b-webui python /app/mcp_server.py

# 4. æŸ¥çœ‹æ—¥å¿—
docker logs llasa-tts-8b-webui | grep MCP
```

### é—®é¢˜ 2: æ–‡ä»¶è·¯å¾„é”™è¯¯

**ç—‡çŠ¶**ï¼š`audio_path` æ‰¾ä¸åˆ°æ–‡ä»¶

**åŸå› **ï¼šMCP åœ¨å®¹å™¨å†…è¿è¡Œï¼Œéœ€è¦ä½¿ç”¨å®¹å™¨å†…çš„è·¯å¾„

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âŒ é”™è¯¯ï¼šä½¿ç”¨å®¿ä¸»æœºè·¯å¾„
"audio_path": "/home/user/audio.wav"

# âœ… æ­£ç¡®ï¼šä½¿ç”¨å®¹å™¨å†…è·¯å¾„
"audio_path": "/app/temp/audio.wav"

# æˆ–è€…å…ˆå¤åˆ¶æ–‡ä»¶åˆ°æŒ‚è½½ç›®å½•
# å®¿ä¸»æœº: ./temp/audio.wav
# å®¹å™¨å†…: /app/temp/audio.wav
```

### é—®é¢˜ 3: å·¥å…·è°ƒç”¨è¶…æ—¶

**ç—‡çŠ¶**ï¼šé•¿æ—¶é—´æ— å“åº”

**åŸå› **ï¼šé¦–æ¬¡åŠ è½½æ¨¡å‹éœ€è¦ 20-30 ç§’

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# å¢åŠ è¶…æ—¶æ—¶é—´
async with ClientSession(timeout=120) as session:  # 120 ç§’è¶…æ—¶
    result = await session.call_tool("generate_speech", {...})
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [GPU ç®¡ç†æ–‡æ¡£](./GPU_MANAGEMENT.md) - GPU æ™ºèƒ½ç®¡ç†è¯¦è§£
- [Docker éƒ¨ç½²æŒ‡å—](./DOCKER_GUIDE.md) - Docker éƒ¨ç½²è¯´æ˜
- [API æ–‡æ¡£](http://localhost:7861/apidocs) - REST API æ¥å£æ–‡æ¡£

## ğŸ¤ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
1. [é¡¹ç›® README](./README.md)
2. [å¸¸è§é—®é¢˜](./FAQ.md)
3. [GitHub Issues](https://github.com/your-repo/issues)

## ğŸ“– å‚è€ƒèµ„æ–™

- [FastMCP å®˜æ–¹æ–‡æ¡£](https://github.com/jlowin/fastmcp)
- [Model Context Protocol è§„èŒƒ](https://modelcontextprotocol.io/)
- [Llasa-8B æ¨¡å‹](https://huggingface.co/HKUSTAudio/Llasa-8B)
