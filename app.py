import os
# è®¾ç½®ä½¿ç”¨ GPU 1 å’Œ 2ï¼ˆç”¨æˆ·æŒ‡å®š 1 å·å’Œ 2 å· GPU æ¯”è¾ƒç©ºé—²ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = os.environ.get('CUDA_VISIBLE_DEVICES', '1,2')

from transformers import AutoTokenizer, AutoModelForCausalLM
from faster_whisper import WhisperModel
import torch
from xcodec2.modeling_xcodec2 import XCodec2Model
import torchaudio
import gradio as gr
import numpy
import random

"""
# Llasa-8B ä¸‹è½½éœ€è¦ hf çš„ tokenï¼Œä¸‹é¢æ˜¯è·å– token å’Œç™»å½•ï¼Œå¦‚æœéœ€è¦å°±å»é™¤æ‰æ³¨é‡Š
# å»ºè®®ç›´æ¥å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°åŠ è½½
import os
api_key = os.getenv("HF_TOKEN")

from huggingface_hub import login
login(token=api_key)

# æ²¡é­”æ³•å°±ç”¨è¿™ä¸ªé•œåƒ
os.environ['HF_ENDPOINT']= 'https://hf-mirror.com'
"""


def set_seed(seed=49):
    numpy.random.seed(seed=seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


# FastWhisper é…ç½®
print("ğŸ”„ æ­£åœ¨åŠ è½½ FastWhisper æ¨¡å‹...")
fastwhisper_path = "Systran/faster-whisper-large-v3"
fastwhisper_model = WhisperModel(fastwhisper_path, device="cpu")
language = None
print("âœ… FastWhisper æ¨¡å‹åŠ è½½å®Œæˆ!")

def fastwhisper_asr_file(audio_file):
    if audio_file is None:
        gr.Warning("è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘!")
        return ""
    text = ""
    try:
        segments, info = fastwhisper_model.transcribe(
            audio=audio_file,
            beam_size=5,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=700),
            language=language
        )
        for segment in segments:
            text += segment.text + "\n"
        if not text.strip():
            gr.Warning("æœªèƒ½è¯†åˆ«å‡ºéŸ³é¢‘ä¸­çš„æ–‡å­—ï¼Œè¯·æ£€æŸ¥éŸ³é¢‘è´¨é‡æˆ–æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬")
    except Exception as e:
        gr.Error(f"è½¬å½•å¤±è´¥: {str(e)}")
        print(f"FastWhisper é”™è¯¯: {e}")
    return text.strip()


# Llasa-8B æ¨¡å‹é…ç½®
print("ğŸ”„ æ­£åœ¨åŠ è½½ XCodec2 æ¨¡å‹...")
model_path = "HKUSTAudio/xcodec2"
Codec_model = XCodec2Model.from_pretrained(model_path, device_map='cuda')
Codec_model.eval()
print("âœ… XCodec2 æ¨¡å‹åŠ è½½å®Œæˆ!")

print("ğŸ”„ æ­£åœ¨åŠ è½½ Llasa-8B æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
llasa_8b = 'HKUSTAudio/Llasa-8B'
tokenizer = AutoTokenizer.from_pretrained(llasa_8b)
llasa_model = AutoModelForCausalLM.from_pretrained(
    llasa_8b,
    torch_dtype=torch.float16,
    device_map='cuda'
)
llasa_model.eval()
print("âœ… Llasa-8B æ¨¡å‹åŠ è½½å®Œæˆ!")

# æ‰“å° GPU ä¿¡æ¯
if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
    print(f"\nğŸ® GPU ä¿¡æ¯:")
    print(f"   å¯ç”¨ GPU æ•°é‡: {gpu_count}")
    for i in range(gpu_count):
        print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    print(f"   å½“å‰ä½¿ç”¨çš„ GPU: {os.environ.get('CUDA_VISIBLE_DEVICES', 'all')}\n")
else:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° CUDA GPU!")


def ids_to_speech_tokens(speech_ids):
    speech_tokens_str = []
    for speech_id in speech_ids:
        speech_tokens_str.append(f"<|s_{speech_id}|>")
    return speech_tokens_str


def extract_speech_ids(speech_tokens_str):
    speech_ids = []
    for token_str in speech_tokens_str:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            num_str = token_str[4:-2]
            num = int(num_str)
            speech_ids.append(num)
        else:
            print(f"Unexpected token: {token_str}")
    return speech_ids


def tts(sample_audio_path, sample_text, system_prompt_text, target_text,
        sample_rate_input=24000, penalty=1.2, temperature=1.0, top_k=50,
        top_p=0.9, do_sample=True, random_seed=49):

    # å‚æ•°éªŒè¯
    if sample_audio_path is None:
        gr.Warning("è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘!")
        return None
    if not sample_text or not sample_text.strip():
        gr.Warning("è¯·è¾“å…¥å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹!")
        return None
    if not target_text or not target_text.strip():
        gr.Warning("è¯·è¾“å…¥è¦ç”Ÿæˆçš„æ–‡å­—!")
        return None

    set_seed(random_seed)
    progress = gr.Progress()

    try:
        progress(0, 'ğŸµ åŠ è½½éŸ³é¢‘...')
        waveform, sample_rate = torchaudio.load(sample_audio_path)

        # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
        audio_duration = len(waveform[0]) / sample_rate
        if audio_duration > 20:
            gr.Warning(f"å‚è€ƒéŸ³é¢‘è¾ƒé•¿ ({audio_duration:.1f}ç§’)ï¼Œå»ºè®®ä½¿ç”¨ 15-20 ç§’çš„éŸ³é¢‘ä»¥è·å¾—æœ€ä½³æ•ˆæœ")

        # ç«‹ä½“å£°è½¬å•å£°é“
        if waveform.size(0) > 1:
            waveform_mono = torch.mean(waveform, dim=0, keepdim=True)
        else:
            waveform_mono = waveform

        prompt_wav = torchaudio.transforms.Resample(
            orig_freq=sample_rate,
            new_freq=sample_rate_input
        )(waveform_mono)

        progress(0.2, 'ğŸ“ å¤„ç†æ–‡æœ¬...')
        if len(target_text) > 300:
            gr.Warning("è¦ç”Ÿæˆçš„æ–‡å­—å¤ªé•¿ï¼Œå·²è‡ªåŠ¨æˆªæ–­åˆ° 300 ä¸ªå­—ç¬¦")
            target_text = target_text[:300]

        input_text = sample_text + ' ' + target_text

        with torch.no_grad():
            progress(0.4, 'ğŸ”Š ç¼–ç éŸ³é¢‘...')
            vq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)
            vq_code_prompt = vq_code_prompt[0, 0, :]
            speech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)

            formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"
            chat = [
                {"role": "user", "content": system_prompt_text + ":" + formatted_text},
                {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>" + ''.join(speech_ids_prefix)}
            ]

            input_ids = tokenizer.apply_chat_template(
                chat,
                tokenize=True,
                return_tensors='pt',
                continue_final_message=True
            )
            input_ids = input_ids.cuda()
            speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')

            progress(0.6, 'ğŸ™ï¸ ç”Ÿæˆè¯­éŸ³ä¸­...')
            outputs = llasa_model.generate(
                input_ids,
                max_length=2048,
                eos_token_id=speech_end_id,
                do_sample=do_sample,
                top_p=top_p,
                top_k=top_k,
                temperature=temperature,
                repetition_penalty=penalty
            )

            generated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]
            speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            speech_tokens = extract_speech_ids(speech_tokens)
            speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)

            progress(0.8, 'ğŸ§ è§£ç éŸ³é¢‘...')
            gen_wav = Codec_model.decode_code(speech_tokens)
            gen_wav = gen_wav[:, :, prompt_wav.shape[1]:]

            progress(1, 'âœ… ç”Ÿæˆå®Œæˆ!')
            return (sample_rate_input, gen_wav[0, 0, :].cpu().numpy())

    except Exception as e:
        gr.Error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        print(f"TTS ç”Ÿæˆé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


# è‡ªå®šä¹‰ CSS æ ·å¼
custom_css = """
#main-container {
    max-width: 1400px;
    margin: auto;
}

.header-text {
    text-align: center;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    font-size: 3em;
    font-weight: bold;
    margin-bottom: 0.5em;
}

.sub-header {
    text-align: center;
    color: #666;
    font-size: 1.2em;
    margin-bottom: 2em;
}

.info-box {
    background-color: #f0f7ff;
    border-left: 4px solid #3b82f6;
    padding: 15px;
    margin: 10px 0;
    border-radius: 5px;
}

.warning-box {
    background-color: #fffbeb;
    border-left: 4px solid #f59e0b;
    padding: 15px;
    margin: 10px 0;
    border-radius: 5px;
}

.section-header {
    font-size: 1.5em;
    font-weight: bold;
    color: #1f2937;
    margin-top: 1.5em;
    margin-bottom: 0.5em;
    padding-bottom: 0.5em;
    border-bottom: 2px solid #e5e7eb;
}

.gradio-container {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

footer {
    margin-top: 3em;
    text-align: center;
    color: #999;
}
"""

# æ„å»ºç•Œé¢
with gr.Blocks(title="LLASA-8B TTS WebUI") as app:
    with gr.Column(elem_id="main-container"):
        # æ ‡é¢˜
        gr.HTML("""
            <div class="header-text">ğŸ™ï¸ LLASA-8B TTS WebUI</div>
            <div class="sub-header">
                åŸºäº Llasa-8B çš„é«˜è´¨é‡è¯­éŸ³åˆæˆç³»ç»Ÿ | æ”¯æŒä¸­è‹±æ–‡æ··åˆè¯­éŸ³å…‹éš†
            </div>
        """)

        # ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ### ğŸš€ å¿«é€Ÿå¼€å§‹

            1. **ä¸Šä¼ å‚è€ƒéŸ³é¢‘**ï¼šä¸Šä¼ ä¸€æ®µ 15-20 ç§’çš„æ¸…æ™°è¯­éŸ³ï¼ˆ.wav æ ¼å¼ï¼‰
            2. **è½¬å½•æˆ–è¾“å…¥æ–‡æœ¬**ï¼šä½¿ç”¨è‡ªåŠ¨è½¬å½•åŠŸèƒ½ï¼Œæˆ–æ‰‹åŠ¨è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­çš„æ–‡å­—
            3. **è¾“å…¥ç›®æ ‡æ–‡æœ¬**ï¼šå¡«å†™ä½ æƒ³è¦ç”Ÿæˆçš„è¯­éŸ³å†…å®¹ï¼ˆæœ€å¤š 300 å­—ï¼‰
            4. **è°ƒæ•´å‚æ•°**ï¼ˆå¯é€‰ï¼‰ï¼šæ ¹æ®éœ€è¦è°ƒæ•´é«˜çº§å‚æ•°
            5. **ç‚¹å‡»ç”Ÿæˆ**ï¼šç­‰å¾…æ¨¡å‹ç”Ÿæˆè¯­éŸ³

            ### âš™ï¸ å‚æ•°è¯´æ˜

            - **Temperature**ï¼šæ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼ˆè¶Šä½è¶Šç¨³å®šï¼Œè¶Šé«˜è¶Šå¤šæ ·ï¼‰
            - **Top K/Top P**ï¼šé‡‡æ ·ç­–ç•¥å‚æ•°
            - **Penalty**ï¼šé‡å¤æƒ©ç½šç³»æ•°
            - **Sample Rate**ï¼šè¾“å‡ºéŸ³é¢‘çš„é‡‡æ ·ç‡

            ### ğŸ’¡ æç¤º

            - å‚è€ƒéŸ³é¢‘è´¨é‡è¶Šé«˜ï¼Œç”Ÿæˆæ•ˆæœè¶Šå¥½
            - å»ºè®®ä½¿ç”¨æ¸…æ™°ã€æ— èƒŒæ™¯å™ªéŸ³çš„éŸ³é¢‘
            - æ–‡æœ¬å†…å®¹åº”ä¸å‚è€ƒéŸ³é¢‘çš„è¯­è¨€é£æ ¼ä¸€è‡´
            """)

        # é«˜çº§è®¾ç½®åŒºåŸŸ
        with gr.Accordion("âš™ï¸ é«˜çº§è®¾ç½®", open=False):
            with gr.Row():
                with gr.Column():
                    random_seed = gr.Number(
                        label="ğŸ² éšæœºç§å­",
                        value=49,
                        minimum=0,
                        maximum=10000000,
                        step=1,
                        info="å›ºå®šç§å­å¯ä»¥ç¡®ä¿ç»“æœå¯å¤ç°"
                    )
                    sample_rate = gr.Dropdown(
                        choices=[16000, 22050, 24000, 28000, 32000],
                        value=24000,
                        label="ğŸµ é‡‡æ ·ç‡ (Hz)",
                        info="å»ºè®®ä½¿ç”¨ 24000 Hz"
                    )
                    do_sample = gr.Checkbox(
                        label="ğŸ¯ å¯ç”¨é‡‡æ ·",
                        value=True,
                        info="å…³é—­åå°†ä½¿ç”¨è´ªå©ªè§£ç "
                    )

                with gr.Column():
                    temperature = gr.Slider(
                        label="ğŸŒ¡ï¸ Temperature",
                        value=1.0,
                        minimum=0.0,
                        maximum=1.5,
                        step=0.05,
                        info="æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§"
                    )
                    top_k = gr.Slider(
                        label="ğŸ“Š Top K",
                        value=50,
                        minimum=1,
                        maximum=100,
                        step=1,
                        info="Top-K é‡‡æ ·å‚æ•°"
                    )
                    top_p = gr.Slider(
                        label="ğŸ“ˆ Top P",
                        value=0.9,
                        minimum=0.0,
                        maximum=1.0,
                        step=0.05,
                        info="Nucleus é‡‡æ ·å‚æ•°"
                    )
                    penalty = gr.Slider(
                        label="ğŸ”„ é‡å¤æƒ©ç½š",
                        value=1.2,
                        minimum=0.0,
                        maximum=2.0,
                        step=0.05,
                        info="æƒ©ç½šé‡å¤å†…å®¹"
                    )

        gr.HTML('<div class="section-header">ğŸ“‚ æ­¥éª¤ 1: ä¸Šä¼ å‚è€ƒéŸ³é¢‘</div>')
        gr.HTML("""
            <div class="info-box">
                â„¹ï¸ <strong>æç¤ºï¼š</strong>è¯·ä¸Šä¼  15-20 ç§’çš„ .wav æ ¼å¼éŸ³é¢‘æ–‡ä»¶ï¼Œç¡®ä¿éŸ³è´¨æ¸…æ™°ã€æ— èƒŒæ™¯å™ªéŸ³
            </div>
        """)

        with gr.Row():
            with gr.Column(scale=1):
                ref_audio_input = gr.Audio(
                    label="ğŸµ å‚è€ƒéŸ³é¢‘æ–‡ä»¶",
                    type="filepath",
                    sources=["upload"]
                )

            with gr.Column(scale=1):
                ref_text_input = gr.Textbox(
                    label="ğŸ“ å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹",
                    lines=5,
                    placeholder="ç‚¹å‡»ã€Œè‡ªåŠ¨è½¬å½•ã€æŒ‰é’®ï¼Œæˆ–æ‰‹åŠ¨è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...",
                    info="å¿…é¡»ä¸å‚è€ƒéŸ³é¢‘å†…å®¹ä¸€è‡´"
                )
                transcribe_btn = gr.Button(
                    "ğŸ¤ è‡ªåŠ¨è½¬å½•ï¼ˆä½¿ç”¨ FastWhisperï¼‰",
                    variant="secondary",
                    size="lg"
                )

        transcribe_btn.click(
            fn=fastwhisper_asr_file,
            inputs=[ref_audio_input],
            outputs=[ref_text_input]
        )

        gr.HTML('<div class="section-header">âœï¸ æ­¥éª¤ 2: è¾“å…¥ç”Ÿæˆå†…å®¹</div>')

        system_prompt_text_input = gr.Textbox(
            label="ğŸ’¬ ç³»ç»Ÿæç¤ºè¯",
            lines=1,
            value="Convert the text to speech",
            info="é€šå¸¸ä¸éœ€è¦ä¿®æ”¹"
        )

        gen_text_input = gr.Textbox(
            label="ğŸ“„ è¦ç”Ÿæˆçš„æ–‡å­—å†…å®¹",
            lines=6,
            placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„è¯­éŸ³å†…å®¹ï¼ˆæœ€å¤š 300 å­—ç¬¦ï¼‰...",
            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆè¾“å…¥"
        )

        generate_btn = gr.Button(
            "ğŸ™ï¸ ç”Ÿæˆè¯­éŸ³",
            variant="primary",
            size="lg"
        )

        gr.HTML('<div class="section-header">ğŸ§ æ­¥éª¤ 3: ç”Ÿæˆç»“æœ</div>')

        audio_output = gr.Audio(
            label="ğŸ”Š ç”Ÿæˆçš„è¯­éŸ³",
            type="numpy"
        )

        generate_btn.click(
            fn=tts,
            inputs=[
                ref_audio_input,
                ref_text_input,
                system_prompt_text_input,
                gen_text_input,
                sample_rate,
                penalty,
                temperature,
                top_k,
                top_p,
                do_sample,
                random_seed
            ],
            outputs=[audio_output],
        )

        # ç¤ºä¾‹
        with gr.Accordion("ğŸ“š æŸ¥çœ‹ç¤ºä¾‹", open=False):
            gr.Examples(
                examples=[
                    [
                        None,
                        "è¿™æ˜¯ä¸€æ®µå‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹ã€‚",
                        "Convert the text to speech",
                        "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨ Llasa è¯­éŸ³åˆæˆç³»ç»Ÿã€‚",
                    ]
                ],
                inputs=[
                    ref_audio_input,
                    ref_text_input,
                    system_prompt_text_input,
                    gen_text_input,
                ],
                label="ç¤ºä¾‹è¾“å…¥"
            )

        # é¡µè„š
        gr.HTML("""
            <footer>
                <p>ğŸ’» Powered by <strong>Llasa-8B</strong> |
                ğŸ”¬ Model: <a href="https://huggingface.co/HKUSTAudio/Llasa-8B" target="_blank">HKUSTAudio/Llasa-8B</a> |
                ğŸ“ License: MIT</p>
                <p style="font-size: 0.9em; color: #999;">
                    âš¡ GPU: {gpu_info}
                </p>
            </footer>
        """.format(
            gpu_info=f"ä½¿ç”¨ GPU {os.environ.get('CUDA_VISIBLE_DEVICES', 'N/A')}"
            if torch.cuda.is_available() else "æœªæ£€æµ‹åˆ° GPU"
        ))


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ å¯åŠ¨ Llasa-8B TTS WebUI")
    print("="*60)
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
