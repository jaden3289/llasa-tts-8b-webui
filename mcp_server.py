"""
Llasa-TTS-8B MCP æœåŠ¡å™¨

Model Context Protocol - ç¨‹åºåŒ–è®¿é—®æ¥å£
ä½¿ç”¨ FastMCP æ¡†æ¶
"""

import os
import sys
import logging
import tempfile
from pathlib import Path
from typing import Optional, Dict, Any
import traceback

import torch
import torchaudio
from transformers import AutoTokenizer, AutoModelForCausalLM
from faster_whisper import WhisperModel
from xcodec2.modeling_xcodec2 import XCodec2Model
from fastmcp import FastMCP

# å¯¼å…¥ GPU ç®¡ç†å™¨
from gpu_manager import get_global_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# åˆå§‹åŒ– MCP æœåŠ¡å™¨
# ============================================================================

mcp = FastMCP("Llasa-TTS-8B")

# GPU ç®¡ç†å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼‰
gpu_manager = get_global_manager(
    idle_timeout=int(os.getenv('GPU_IDLE_TIMEOUT', 600))
)

# æ¨¡å‹è·¯å¾„é…ç½®
LLASA_MODEL_PATH = os.getenv('LLASA_MODEL_PATH', 'HKUSTAudio/Llasa-8B')
XCODEC_MODEL_PATH = os.getenv('XCODEC_MODEL_PATH', 'HKUSTAudio/xcodec2')
WHISPER_MODEL_PATH = os.getenv('WHISPER_MODEL_PATH', 'Systran/faster-whisper-large-v3')
WHISPER_DEVICE = os.getenv('WHISPER_DEVICE', 'cpu')

# å…¨å±€å˜é‡
whisper_model = None
llasa_tokenizer = None

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(os.getenv('OUTPUTS_DIR', './outputs'))
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

# ============================================================================
# æ¨¡å‹åŠ è½½å‡½æ•°
# ============================================================================

def load_llasa_model():
    """åŠ è½½ Llasa-8B æ¨¡å‹"""
    logger.info(f"ğŸ”„ åŠ è½½ Llasa-8B: {LLASA_MODEL_PATH}")
    model = AutoModelForCausalLM.from_pretrained(
        LLASA_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map='cuda'
    )
    model.eval()
    return model


def load_xcodec_model():
    """åŠ è½½ XCodec2 æ¨¡å‹"""
    logger.info(f"ğŸ”„ åŠ è½½ XCodec2: {XCODEC_MODEL_PATH}")
    model = XCodec2Model.from_pretrained(XCODEC_MODEL_PATH, device_map='cuda')
    model.eval()
    return model


def get_whisper_model():
    """è·å– Whisper æ¨¡å‹"""
    global whisper_model
    if whisper_model is None:
        logger.info(f"ğŸ”„ åŠ è½½ Whisper: {WHISPER_MODEL_PATH}")
        whisper_model = WhisperModel(WHISPER_MODEL_PATH, device=WHISPER_DEVICE)
    return whisper_model


def get_tokenizer():
    """è·å– Tokenizer"""
    global llasa_tokenizer
    if llasa_tokenizer is None:
        logger.info(f"ğŸ”„ åŠ è½½ Tokenizer: {LLASA_MODEL_PATH}")
        llasa_tokenizer = AutoTokenizer.from_pretrained(LLASA_MODEL_PATH)
    return llasa_tokenizer


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def ids_to_speech_tokens(speech_ids):
    """å°† speech IDs è½¬æ¢ä¸º token å­—ç¬¦ä¸²"""
    return [f"<|s_{sid}|>" for sid in speech_ids]


def extract_speech_ids(speech_tokens_str):
    """ä» token å­—ç¬¦ä¸²æå– speech IDs"""
    speech_ids = []
    for token_str in speech_tokens_str:
        if token_str.startswith('<|s_') and token_str.endswith('|>'):
            num_str = token_str[4:-2]
            speech_ids.append(int(num_str))
    return speech_ids


# ============================================================================
# MCP å·¥å…·å‡½æ•°
# ============================================================================

@mcp.tool()
def generate_speech(
    audio_path: str,
    ref_text: str,
    target_text: str,
    output_path: Optional[str] = None,
    system_prompt: str = "Convert the text to speech",
    sample_rate: int = 24000,
    temperature: float = 1.0,
    top_k: int = 50,
    top_p: float = 0.9,
    penalty: float = 1.2,
    random_seed: int = 49
) -> Dict[str, Any]:
    """
    ç”Ÿæˆè¯­éŸ³å…‹éš†ï¼ˆTTSï¼‰

    Args:
        audio_path: å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„
        ref_text: å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬å†…å®¹
        target_text: è¦ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        sample_rate: é‡‡æ ·ç‡ï¼ˆ16000, 22050, 24000, 28000, 32000ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.5ï¼‰
        top_k: Top-K é‡‡æ ·ï¼ˆ1-100ï¼‰
        top_p: Nucleus é‡‡æ ·ï¼ˆ0.0-1.0ï¼‰
        penalty: é‡å¤æƒ©ç½šï¼ˆ0.0-2.0ï¼‰
        random_seed: éšæœºç§å­

    Returns:
        åŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸
    """
    try:
        logger.info(f"ğŸ™ï¸ å¼€å§‹ç”Ÿæˆè¯­éŸ³: {target_text[:50]}...")

        # éªŒè¯å‚æ•°
        if not Path(audio_path).exists():
            return {
                'status': 'error',
                'error': f'å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}'
            }

        if not ref_text.strip() or not target_text.strip():
            return {
                'status': 'error',
                'error': 'å‚è€ƒæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬ä¸èƒ½ä¸ºç©º'
            }

        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(random_seed)
        torch.cuda.manual_seed_all(random_seed)

        # åŠ è½½å’Œå¤„ç†éŸ³é¢‘
        waveform, sr = torchaudio.load(audio_path)

        if waveform.size(0) > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        prompt_wav = torchaudio.transforms.Resample(
            orig_freq=sr,
            new_freq=sample_rate
        )(waveform)

        input_text = ref_text + ' ' + target_text

        # è·å–æ¨¡å‹ç®¡ç†å™¨
        codec_manager = gpu_manager.register_model("XCodec2")
        llasa_manager = gpu_manager.register_model("Llasa-8B")

        try:
            # === é˜¶æ®µ 1: ç¼–ç éŸ³é¢‘ ===
            codec_model = codec_manager.get_model(load_xcodec_model, "XCodec2")

            with torch.no_grad():
                vq_code_prompt = codec_model.encode_code(input_waveform=prompt_wav)
                vq_code_prompt = vq_code_prompt[0, 0, :]
                speech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)

            codec_manager.force_offload()
            logger.info("âœ… éŸ³é¢‘ç¼–ç å®Œæˆ")

            # === é˜¶æ®µ 2: ç”Ÿæˆè¯­éŸ³ token ===
            llasa_model = llasa_manager.get_model(load_llasa_model, "Llasa-8B")
            tokenizer = get_tokenizer()

            formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"
            chat = [
                {"role": "user", "content": system_prompt + ":" + formatted_text},
                {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>" + ''.join(speech_ids_prefix)}
            ]

            input_ids = tokenizer.apply_chat_template(
                chat,
                tokenize=True,
                return_tensors='pt',
                continue_final_message=True
            ).cuda()

            speech_end_id = tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')

            with torch.no_grad():
                outputs = llasa_model.generate(
                    input_ids,
                    max_length=2048,
                    eos_token_id=speech_end_id,
                    do_sample=True,
                    top_p=top_p,
                    top_k=top_k,
                    temperature=temperature,
                    repetition_penalty=penalty
                )

            generated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]
            speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
            speech_tokens = extract_speech_ids(speech_tokens)
            speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)

            llasa_manager.force_offload()
            logger.info("âœ… è¯­éŸ³ token ç”Ÿæˆå®Œæˆ")

            # === é˜¶æ®µ 3: è§£ç éŸ³é¢‘ ===
            codec_model = codec_manager.get_model(load_xcodec_model, "XCodec2")

            with torch.no_grad():
                gen_wav = codec_model.decode_code(speech_tokens)
                gen_wav = gen_wav[:, :, prompt_wav.shape[1]:]

            codec_manager.force_offload()
            logger.info("âœ… éŸ³é¢‘è§£ç å®Œæˆ")

            # ä¿å­˜éŸ³é¢‘
            if output_path is None:
                import time
                output_path = str(OUTPUT_DIR / f"generated_{int(time.time())}.wav")

            output_path = str(Path(output_path).absolute())
            torchaudio.save(output_path, gen_wav[0].cpu(), sample_rate)

            logger.info(f"ğŸ‰ è¯­éŸ³ç”ŸæˆæˆåŠŸ: {output_path}")

            return {
                'status': 'success',
                'output_path': output_path,
                'sample_rate': sample_rate,
                'duration_seconds': gen_wav.shape[-1] / sample_rate
            }

        except Exception as e:
            # ç¡®ä¿å¼‚å¸¸æ—¶å¸è½½æ¨¡å‹
            codec_manager.force_offload()
            llasa_manager.force_offload()
            raise e

    except Exception as e:
        error_msg = f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        return {
            'status': 'error',
            'error': error_msg
        }


@mcp.tool()
def transcribe_audio(audio_path: str) -> Dict[str, Any]:
    """
    è½¬å½•éŸ³é¢‘ï¼ˆASRï¼‰

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

    Returns:
        åŒ…å«è½¬å½•ç»“æœçš„å­—å…¸
    """
    try:
        if not Path(audio_path).exists():
            return {
                'status': 'error',
                'error': f'éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}'
            }

        logger.info(f"ğŸ¤ å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path}")

        whisper = get_whisper_model()
        segments, info = whisper.transcribe(
            audio=audio_path,
            beam_size=5,
            vad_filter=True,
            vad_parameters=dict(min_silence_duration_ms=700)
        )

        text = ""
        for segment in segments:
            text += segment.text + "\n"

        logger.info(f"âœ… è½¬å½•å®Œæˆ: {len(text)} å­—ç¬¦")

        return {
            'status': 'success',
            'text': text.strip(),
            'language': info.language if hasattr(info, 'language') else 'unknown'
        }

    except Exception as e:
        error_msg = f"è½¬å½•å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return {
            'status': 'error',
            'error': error_msg
        }


@mcp.tool()
def get_gpu_status() -> Dict[str, Any]:
    """
    è·å– GPU çŠ¶æ€

    Returns:
        GPU çŠ¶æ€ä¿¡æ¯
    """
    try:
        status = gpu_manager.get_all_status()
        return {
            'status': 'success',
            'data': status
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }


@mcp.tool()
def offload_gpu() -> Dict[str, str]:
    """
    æ‰‹åŠ¨å¸è½½æ‰€æœ‰æ¨¡å‹åˆ° CPUï¼ˆé‡Šæ”¾ GPU æ˜¾å­˜ï¼‰

    Returns:
        æ“ä½œç»“æœ
    """
    try:
        logger.info("ğŸ“¥ æ‰‹åŠ¨å¸è½½æ‰€æœ‰æ¨¡å‹åˆ° CPU...")
        gpu_manager.offload_all()
        return {
            'status': 'success',
            'message': 'æ‰€æœ‰æ¨¡å‹å·²å¸è½½åˆ° CPUï¼ŒGPU æ˜¾å­˜å·²é‡Šæ”¾'
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }


@mcp.tool()
def release_gpu() -> Dict[str, str]:
    """
    å®Œå…¨é‡Šæ”¾æ‰€æœ‰æ¨¡å‹ï¼ˆæ¸…ç©º GPU å’Œ CPU ç¼“å­˜ï¼‰

    Returns:
        æ“ä½œç»“æœ
    """
    try:
        logger.info("ğŸ—‘ï¸  å®Œå…¨é‡Šæ”¾æ‰€æœ‰æ¨¡å‹...")
        gpu_manager.release_all()
        return {
            'status': 'success',
            'message': 'æ‰€æœ‰æ¨¡å‹å·²å®Œå…¨é‡Šæ”¾'
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }


@mcp.tool()
def update_gpu_timeout(timeout_seconds: int) -> Dict[str, str]:
    """
    æ›´æ–° GPU ç©ºé—²è¶…æ—¶æ—¶é—´

    Args:
        timeout_seconds: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ“ä½œç»“æœ
    """
    try:
        if timeout_seconds < 0:
            return {
                'status': 'error',
                'error': 'è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äºç­‰äº 0'
            }

        gpu_manager.update_all_timeout(timeout_seconds)
        return {
            'status': 'success',
            'message': f'GPU ç©ºé—²è¶…æ—¶å·²æ›´æ–°ä¸º {timeout_seconds} ç§’'
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("ğŸš€ å¯åŠ¨ Llasa-TTS-8B MCP æœåŠ¡å™¨")
    logger.info("=" * 60)
    logger.info(f"ğŸ“¦ å¯ç”¨å·¥å…·:")
    logger.info("   â€¢ generate_speech    - ç”Ÿæˆè¯­éŸ³å…‹éš†")
    logger.info("   â€¢ transcribe_audio   - è½¬å½•éŸ³é¢‘")
    logger.info("   â€¢ get_gpu_status     - è·å– GPU çŠ¶æ€")
    logger.info("   â€¢ offload_gpu        - å¸è½½æ¨¡å‹åˆ° CPU")
    logger.info("   â€¢ release_gpu        - å®Œå…¨é‡Šæ”¾æ¨¡å‹")
    logger.info("   â€¢ update_gpu_timeout - æ›´æ–°è¶…æ—¶æ—¶é—´")
    logger.info("=" * 60)
    logger.info("")

    # è¿è¡Œ MCP æœåŠ¡å™¨
    mcp.run()
