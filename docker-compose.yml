version: '3.8'

services:
  llasa-tts-webui:
    build:
      context: .
      dockerfile: Dockerfile
    image: llasa-tts-8b:latest
    container_name: llasa-tts-8b-webui

    # GPU 配置 - 由 start.sh 通过环境变量动态设置
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${NVIDIA_VISIBLE_DEVICES:-0}']  # 默认使用 GPU 0
              capabilities: [gpu]

    environment:
      # GPU 设置
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}

      # 服务配置
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=${UI_PORT:-7860}
      - API_SERVER_PORT=${API_PORT:-7861}

      # HuggingFace 配置
      - HF_ENDPOINT=${HF_ENDPOINT:-https://hf-mirror.com}
      - HF_TOKEN=${HF_TOKEN:-}

      # GPU 管理配置
      - GPU_IDLE_TIMEOUT=${GPU_IDLE_TIMEOUT:-600}

      # 模型路径（可选，如果使用本地模型）
      - LLASA_MODEL_PATH=${LLASA_MODEL_PATH:-HKUSTAudio/Llasa-8B}
      - XCODEC_MODEL_PATH=${XCODEC_MODEL_PATH:-HKUSTAudio/xcodec2}
      - WHISPER_MODEL_PATH=${WHISPER_MODEL_PATH:-Systran/faster-whisper-large-v3}

    ports:
      - "${UI_PORT:-7860}:7860"      # Gradio WebUI
      - "${API_PORT:-7861}:7861"     # REST API
      - "${MCP_PORT:-7862}:7862"     # MCP Server (if needed)

    volumes:
      # 模型缓存目录（避免每次重启重新下载模型）
      - ${MODELS_CACHE_DIR:-./models_cache}:/root/.cache/huggingface

      # 输出目录
      - ${OUTPUTS_DIR:-./outputs}:/app/outputs

      # 临时文件目录
      - ${TEMP_DIR:-./temp}:/app/temp

      # 可选：挂载本地模型目录
      # - /path/to/your/local/models:/models

    restart: unless-stopped

    # 健康检查
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

    # 资源限制
    mem_limit: ${MEM_LIMIT:-32g}
    shm_size: ${SHM_SIZE:-8g}  # 增加共享内存，有助于 GPU 操作

    # 日志配置
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
